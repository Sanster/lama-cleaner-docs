import { Callout } from "nextra-theme-docs"

# pip

The easiest way to install Lama Cleaner is to install it via `pip`.

```shell
pip install lama-cleaner
```

<Callout type="info" emoji="ℹ️">
  If pip install failed please check you python version is between 3.7 ~ 3.10
</Callout>

After finishing the installation, you can start the server by `lama-cleaner` command,
model file will be downloaded at first time used.

```bash
lama-cleaner --model=lama --device=cpu --port=8080
```

Lama Cleaner is now running at http://localhost:8080

<Callout type="info" emoji="ℹ️">
  If `lama-cleaner` is not found in your terminal, it might because the python
  binary directory not exist in the shell's `PATH`, please check this
  [link](https://github.com/Sanster/lama-cleaner/issues/80#issuecomment-1305701870).
</Callout>

<Callout type="info" emoji="ℹ️">
  If Lama Cleaner is not using GPU device, it might CPU version of pytorch is
  installed, please follow pytorch's
  [get-started](https://pytorch.org/get-started/locally/) to install GPU
  version.
</Callout>

# All command line args

```bash
usage: main.py [-h] [--host HOST] [--port PORT] [--config-installer] [--load-installer-config] [--installer-config INSTALLER_CONFIG] [--model {lama,ldm,zits,mat,fcf,sd1.5,cv2,manga,sd2,paint_by_example,instruct_pix2pix}] [--no-half]
               [--cpu-offload] [--disable-nsfw] [--sd-cpu-textencoder] [--local-files-only] [--enable-xformers] [--device {cuda,cpu,mps}] [--gui] [--no-gui-auto-close] [--gui-size GUI_SIZE GUI_SIZE] [--input INPUT]
               [--output-dir OUTPUT_DIR] [--model-dir MODEL_DIR] [--disable-model-switch]

optional arguments:
  -h, --help            show this help message and exit
  --host HOST
  --port PORT
  --config-installer    Open config web page, mainly for windows installer (default: False)
  --load-installer-config
                        Load all cmd args from installer config file (default: False)
  --installer-config INSTALLER_CONFIG
                        Config file for windows installer (default: None)
  --model {lama,ldm,zits,mat,fcf,sd1.5,cv2,manga,sd2,paint_by_example,instruct_pix2pix}
  --no-half             Using full precision model. If your generate result is always black or green, use this argument. (sd/paint_by_exmaple) (default: False)
  --cpu-offload         Offloads all models to CPU, significantly reducing vRAM usage. (sd/paint_by_example) (default: False)
  --disable-nsfw        Disable NSFW checker. (sd/paint_by_example) (default: False)
  --sd-cpu-textencoder  Run Stable Diffusion text encoder model on CPU to save GPU memory. (default: False)
  --local-files-only    Use local files only, not connect to Hugging Face server. (sd/paint_by_example) (default: False)
  --enable-xformers     Enable xFormers optimizations. Requires xformers package has been installed. See: https://github.com/facebookresearch/xformers (sd/paint_by_example) (default: False)
  --device {cuda,cpu,mps}
  --gui                 Launch Lama Cleaner as desktop app (default: False)
  --no-gui-auto-close   Prevent backend auto close after the GUI window closed. (default: False)
  --gui-size GUI_SIZE GUI_SIZE
                        Set window size for GUI (default: [1600, 1000])
  --input INPUT         If input is image, it will be loaded by default. If input is directory, you can browse and select image in file manager. (default: None)
  --output-dir OUTPUT_DIR
                        Result images will be saved to output directory automatically without confirmation. (default: None)
  --model-dir MODEL_DIR
                        Model download directory (by setting XDG_CACHE_HOME environment variable), by default model downloaded to ~/.cache (default: /Users/cwq/.cache)
  --disable-model-switch
                        Disable model switch in frontend (default: False)
```
