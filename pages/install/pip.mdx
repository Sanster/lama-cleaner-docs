import { Callout } from "nextra-theme-docs";

# pip

The easiest way to install Lama Cleaner is to install it via `pip`.

```shell
pip install lama-cleaner
```

<Callout type="info" emoji="ℹ️">
  If pip install failed please check you python version is between 3.7 ~ 3.10
</Callout>

After finishing the installation, you can start the server by `lama-cleaner` command,
model file will be downloaded at first time used.

```bash
lama-cleaner --model=lama --device=cpu --port=8080
```

Lama Cleaner is now running at http://localhost:8080

<Callout type="info" emoji="ℹ️">
  If `lama-cleaner` is not found in your terminal, it might because the python
  binary directory not exist in the shell's `PATH`, please check this
  [link](https://github.com/Sanster/lama-cleaner/issues/80#issuecomment-1305701870).
</Callout>

<Callout type="info" emoji="ℹ️">
  If Lama Cleaner is not using GPU device, it might CPU version of pytorch is
  installed, please follow pytorch's
  [get-started](https://pytorch.org/get-started/locally/) to install GPU
  version.
</Callout>

# All command line args

| Name                 | Description                                                                                                         | Default   |
| -------------------- | ------------------------------------------------------------------------------------------------------------------- | --------- |
| --model              | lama/ldm/zits/mat/fcf/sd1.5/manga/sd2/paint_by_example                                                              | lama      |
| --model-dir          | Model download directory (by setting XDG_CACHE_HOME environment variable), by default model downloaded to ~/.cache" | lama      |
| --device             | cuda / cpu / mps                                                                                                    | cuda      |
| --sd-disable-nsfw    | Disable stable-diffusion NSFW checker.                                                                              |           |
| --sd-cpu-textencoder | Always run stable-diffusion TextEncoder model on CPU.                                                               |           |
| --sd-enable-xformers | Enable xFormers optimizations. See: [facebookresearch/xformers](https://github.com/facebookresearch/xformers)       |           |
| --local-files-only   | Once the model as downloaded, you can pass this arg to avoid diffusers connect to Hugging Face server               |           |
| --cpu-offload        | sd/paint_by_example model. With cpu offload sd model only needs 2GB GPU memory (image size 512x512)                 |           |
| --no-half            | Using full precision for sd/paint_by_exmaple model. If your generate result is always, use this                     |           |
| --host               | Set to `0.0.0.0` if you want to visit the server on another device                                                  | localhost |
| --port               | Port for backend flask web server                                                                                   | 8080      |
| --gui                | Launch lama-cleaner as a desktop application                                                                        |           |
| --gui_size           | Set the window size for the application                                                                             | 1200 900  |
| --input              | Path to image/directory you want to load by default                                                                 | None      |
| --debug              | Enable debug mode for flask web server                                                                              |           |
